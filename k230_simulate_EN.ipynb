{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/curioyang/utils/blob/master/k230_simulate_EN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c8a2f0-423b-4c62-8cf0-235e881e1d35",
      "metadata": {
        "id": "38c8a2f0-423b-4c62-8cf0-235e881e1d35"
      },
      "source": [
        "If you have any questions, you can join the `QQ group:790699378`, or `create an issue` in the nncase repo:[click here](https://github.com/kendryte/nncase/issues)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/kendryte/nncase.git\n",
        "! cd nncase && git lfs pull"
      ],
      "metadata": {
        "id": "duIJSdK5VQ3V"
      },
      "id": "duIJSdK5VQ3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "4beb4c04-ab79-4e1a-a7ad-ba53d9a9b123",
      "metadata": {
        "id": "4beb4c04-ab79-4e1a-a7ad-ba53d9a9b123"
      },
      "source": [
        "# 1. Install libs and set python env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a8f9c1-c2bf-4270-9f1f-ac25c9fdd898",
      "metadata": {
        "id": "82a8f9c1-c2bf-4270-9f1f-ac25c9fdd898"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install nncase --timeout=1000\n",
        "!pip install nncase-kpu --timeout=1000\n",
        "!pip install onnx onnxsim scikit-learn\n",
        "\n",
        "# # nncase-2.x need dotnet-7\n",
        "# # Ubuntu use apt to install dotnet-7.0 (The docker has installed dotnet7.0)\n",
        "!sudo apt-get install -y dotnet-sdk-7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a17fa3cf",
      "metadata": {
        "id": "a17fa3cf"
      },
      "source": [
        "## auto set enviroment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cdadfc6",
      "metadata": {
        "id": "7cdadfc6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "\n",
        "result = subprocess.run([\"pip\", \"show\", \"nncase\"], capture_output=True)\n",
        "\n",
        "split_flag = \"\\n\"\n",
        "if sys.platform == \"win32\":\n",
        "    split_flag = \"\\r\\n\"\n",
        "\n",
        "location_s = [i for i in result.stdout.decode().split(split_flag) if i.startswith(\"Location:\")]\n",
        "location = location_s[0].split(\": \")[1]\n",
        "\n",
        "if \"PATH\" in os.environ:\n",
        "    os.environ[\"PATH\"] += os.pathsep + location\n",
        "else:\n",
        "    os.environ[\"PATH\"] = location\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b89f3faf-bab6-4f74-a658-1f27a0e49912",
      "metadata": {
        "id": "b89f3faf-bab6-4f74-a658-1f27a0e49912"
      },
      "source": [
        "# 2. Set compile options and PTQ options (quantize model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09bb9ca5-f086-45d8-9ba5-c2415f24e167",
      "metadata": {
        "id": "09bb9ca5-f086-45d8-9ba5-c2415f24e167"
      },
      "source": [
        "You can find more details about [preprocess args](https://github.com/kendryte/nncase/blob/master/docs/USAGE_v2_EN.md#CompileOptions), [quantize options](https://github.com/kendryte/nncase/blob/master/docs/USAGE_v2_EN.md#PTQTensorOptions) and [Mix quantize](https://github.com/kendryte/nncase/blob/master/docs/MixQuant.md)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import onnx\n",
        "import onnxsim\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nncase\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    \"\"\"\n",
        "    result compare\n",
        "    \"\"\"\n",
        "    return cosine_similarity(vec1.reshape(1, -1), vec2.reshape(1, -1))\n",
        "\n",
        "\n",
        "def read_model_file(model_file):\n",
        "    \"\"\"\n",
        "    read model\n",
        "    \"\"\"\n",
        "    with open(model_file, 'rb') as f:\n",
        "        model_content = f.read()\n",
        "    return model_content\n",
        "\n",
        "\n",
        "def parse_model_input_output(model_file):\n",
        "    \"\"\"\n",
        "    parse onnx model\n",
        "    \"\"\"\n",
        "    onnx_model = onnx.load(model_file)\n",
        "    input_all = [node.name for node in onnx_model.graph.input]\n",
        "    input_initializer = [node.name for node in onnx_model.graph.initializer]\n",
        "    input_names = list(set(input_all) - set(input_initializer))\n",
        "    input_tensors = [\n",
        "        node for node in onnx_model.graph.input if node.name in input_names]\n",
        "\n",
        "    # input\n",
        "    inputs = []\n",
        "    for _, e in enumerate(input_tensors):\n",
        "        onnx_type = e.type.tensor_type\n",
        "        input_dict = {}\n",
        "        input_dict['name'] = e.name\n",
        "        input_dict['dtype'] = onnx.mapping.TENSOR_TYPE_TO_NP_TYPE[onnx_type.elem_type]\n",
        "        input_dict['shape'] = [i.dim_value for i in onnx_type.shape.dim]\n",
        "        inputs.append(input_dict)\n",
        "\n",
        "    return onnx_model, inputs\n",
        "\n",
        "def model_simplify(model_file):\n",
        "    \"\"\"\n",
        "    simplify onnx model\n",
        "    \"\"\"\n",
        "    if model_file.split('.')[-1] == \"onnx\":\n",
        "        onnx_model, inputs = parse_model_input_output(model_file)\n",
        "        onnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n",
        "        input_shapes = {}\n",
        "        for input in inputs:\n",
        "            input_shapes[input['name']] = input['shape']\n",
        "\n",
        "        onnx_model, check = onnxsim.simplify(onnx_model, input_shapes=input_shapes)\n",
        "        assert check, \"Simplified ONNX model could not be validated\"\n",
        "\n",
        "        model_file = os.path.join(os.path.dirname(model_file), 'simplified.onnx')\n",
        "        onnx.save_model(onnx_model, model_file)\n",
        "        print(\"[ onnx done ]\")\n",
        "    elif model_file.split('.')[-1] == \"tflite\":\n",
        "        print(\"[ tflite skip ]\")\n",
        "    else:\n",
        "        raise Exception(f\"Unsupport type {model_file.split('.')[-1]}\")\n",
        "\n",
        "    return model_file\n",
        "\n",
        "def run_kmodel(kmodel_path, input_data):\n",
        "    print(\"\\n---------start run kmodel---------\")\n",
        "    print(\"Load kmodel...\")\n",
        "    model_sim = nncase.Simulator()\n",
        "    with open(kmodel_path, 'rb') as f:\n",
        "        model_sim.load_model(f.read())\n",
        "\n",
        "    print(\"Set input data...\")\n",
        "    for i, p_d in enumerate(input_data):\n",
        "        model_sim.set_input_tensor(i, nncase.RuntimeTensor.from_numpy(p_d))\n",
        "\n",
        "    print(\"Run...\")\n",
        "    model_sim.run()\n",
        "\n",
        "    print(\"Get output result...\")\n",
        "    all_result = []\n",
        "    for i in range(model_sim.outputs_size):\n",
        "        result = model_sim.get_output_tensor(i).to_numpy()\n",
        "        all_result.append(result)\n",
        "    print(\"----------------end-----------------\")\n",
        "    return all_result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TNb-Zc8Lmaov"
      },
      "id": "TNb-Zc8Lmaov",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7eff82e-295c-4cce-afbc-ce64c84dc40a",
      "metadata": {
        "id": "a7eff82e-295c-4cce-afbc-ce64c84dc40a"
      },
      "outputs": [],
      "source": [
        "import nncase\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "def compile_kmodel(model_path, dump_path, calib_data):\n",
        "    \"\"\"\n",
        "    Set compile options and ptq options.\n",
        "    Compile kmodel.\n",
        "    Dump the compile-time result to 'compile_options.dump_dir'\n",
        "    \"\"\"\n",
        "    print(\"\\n----------   compile    ----------\")\n",
        "    print(\"Simplify...\")\n",
        "    model_file = model_simplify(model_path)\n",
        "\n",
        "    print(\"Set options...\")\n",
        "    # import_options\n",
        "    import_options = nncase.ImportOptions()\n",
        "\n",
        "    ############################################\n",
        "    # The code below, you need to modify to fit your model.\n",
        "    # You can find more details about these options in docs/USAGE_v2.md.\n",
        "    ############################################\n",
        "    # compile_options\n",
        "    compile_options = nncase.CompileOptions()\n",
        "    compile_options.target = \"k230\" #\"cpu\"\n",
        "    compile_options.dump_ir = False  # if False, will not dump the compile-time result.\n",
        "    compile_options.dump_asm = True\n",
        "    compile_options.dump_dir = dump_path\n",
        "    compile_options.input_file = \"\"\n",
        "\n",
        "    # preprocess args\n",
        "    compile_options.preprocess = False\n",
        "    if compile_options.preprocess:\n",
        "        compile_options.input_type = \"uint8\" # \"uint8\" \"float32\"\n",
        "        compile_options.input_shape = [1,224,320,3]\n",
        "        compile_options.input_range = [0,1]\n",
        "        compile_options.input_layout = \"NHWC\" # \"NHWC\"\n",
        "        compile_options.swapRB = False\n",
        "        compile_options.mean = [0,0,0]\n",
        "        compile_options.std = [1,1,1]\n",
        "        compile_options.letterbox_value = 0\n",
        "        compile_options.output_layout = \"NHWC\" # \"NHWC\"\n",
        "\n",
        "    # quantize options\n",
        "    ptq_options = nncase.PTQTensorOptions()\n",
        "    ptq_options.quant_type = \"uint8\" # datatype : \"float32\", \"int8\", \"int16\"\n",
        "    ptq_options.w_quant_type = \"uint8\"  # datatype : \"float32\", \"int8\", \"int16\"\n",
        "    ptq_options.calibrate_method = \"NoClip\" # \"Kld\"\n",
        "    ptq_options.finetune_weights_method = \"NoFineTuneWeights\"\n",
        "    ptq_options.dump_quant_error = False\n",
        "    ptq_options.dump_quant_error_symmetric_for_signed = False\n",
        "\n",
        "    # mix quantize options\n",
        "    # more details in docs/MixQuant.md\n",
        "    ptq_options.quant_scheme = \"\"\n",
        "    ptq_options.export_quant_scheme = False\n",
        "    ptq_options.export_weight_range_by_channel = False\n",
        "    ############################################\n",
        "\n",
        "    ptq_options.samples_count = len(calib_data[0])\n",
        "    ptq_options.set_tensor_data(calib_data)\n",
        "\n",
        "    print(\"Compiling...\")\n",
        "    compiler = nncase.Compiler(compile_options)\n",
        "    # import\n",
        "    model_content = read_model_file(model_file)\n",
        "    if model_path.split(\".\")[-1] == \"onnx\":\n",
        "        compiler.import_onnx(model_content, import_options)\n",
        "    elif model_path.split(\".\")[-1] == \"tflite\":\n",
        "        compiler.import_tflite(model_content, import_options)\n",
        "\n",
        "    compiler.use_ptq(ptq_options)\n",
        "\n",
        "    # compile\n",
        "    compiler.compile()\n",
        "    kmodel = compiler.gencode_tobytes()\n",
        "\n",
        "    kmodel_path = os.path.join(dump_path, \"test.kmodel\")\n",
        "    with open(kmodel_path, 'wb') as f:\n",
        "        f.write(kmodel)\n",
        "    print(\"----------------end-----------------\")\n",
        "    return kmodel_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e248f46c-fb41-4efc-88b1-bd7f8adbccbb",
      "metadata": {
        "id": "e248f46c-fb41-4efc-88b1-bd7f8adbccbb"
      },
      "source": [
        "# 3. Compile model with a single input.\n",
        "Before all, we need to get the input and output infos of model. <br>\n",
        "Open [netron](https://netron.app), and click 'open model' to select your model.<br>\n",
        "Click the input node, you will see the details of model's input and output.<br>\n",
        "![image.png](attachment:1855346b-3785-4867-9c92-0e811d00b9ab.png) <br>\n",
        "\n",
        "Before compiling kmodel, we should set the `CompileOptions` and `PTQTensorOptions` to specify the attribute of kmodel.\n",
        "\n",
        "Then, we can set the `model_path`, `dump_path`, and `calib_data`. <br>\n",
        "The calib_data format is `[[x1, x2,...]]`. <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c957fe20-99c9-4a54-bae8-38361a8f8830",
      "metadata": {
        "id": "c957fe20-99c9-4a54-bae8-38361a8f8830"
      },
      "outputs": [],
      "source": [
        "# compile kmodel single input\n",
        "model_path = \"/content/nncase/examples/user_guide/test.tflite\"\n",
        "dump_path = \"/content/tmp_tflite\"\n",
        "\n",
        "# sample_count is 2\n",
        "calib_data = [[np.random.rand(1, 240, 320, 3).astype(np.float32), np.random.rand(1, 240, 320, 3).astype(np.float32)]]\n",
        "\n",
        "kmodel_path = compile_kmodel(model_path, dump_path, calib_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "015b6422-3bf1-4f28-93c9-dc6ef6e27856",
      "metadata": {
        "id": "015b6422-3bf1-4f28-93c9-dc6ef6e27856"
      },
      "source": [
        "# 4. Simulate kmodel with a single input.\n",
        "Set `kmodel_path` and `input_data`. After running, it will print shape of result. And result will be stored in `dump_path`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f617edc-781c-4b8b-b45d-fef2f0b36a46",
      "metadata": {
        "id": "7f617edc-781c-4b8b-b45d-fef2f0b36a46"
      },
      "outputs": [],
      "source": [
        "# run kmodel(simulate)\n",
        "import os\n",
        "\n",
        "kmodel_path = \"./tmp_tflite/test.kmodel\"\n",
        "input_data = [np.random.rand(1, 240, 320, 3).astype(np.float32)]\n",
        "\n",
        "result = run_kmodel(kmodel_path, input_data)\n",
        "\n",
        "for idx, i in enumerate(result):\n",
        "    print(i.shape)\n",
        "    i.tofile(os.path.join(dump_path,\"nncase_result_{}.bin\".format(idx)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80216eab-2738-4167-ba03-7a0e218c8d5c",
      "metadata": {
        "id": "80216eab-2738-4167-ba03-7a0e218c8d5c"
      },
      "source": [
        "# 5. Compare kmodel result and tflite result.\n",
        "\n",
        "Here, we will use the TensorFlow framework to infer model(`.tflite`, not kmodel). And calculate the cosine between the tflite result and kmodel result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4003a7ba-a1b4-4488-b3ca-d9a00a55e964",
      "metadata": {
        "id": "4003a7ba-a1b4-4488-b3ca-d9a00a55e964"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "interp = tf.lite.Interpreter(model_path=model_path)\n",
        "\n",
        "inputs = []\n",
        "for idx, item in enumerate(interp.get_input_details()):\n",
        "    input_dict = {}\n",
        "    input_dict['index'] = item['index']\n",
        "    input_dict['value'] = input_data[idx]\n",
        "    inputs.append(input_dict)\n",
        "\n",
        "# print(input_dict)\n",
        "\n",
        "interp.allocate_tensors()\n",
        "for input in inputs:\n",
        "    interp.set_tensor(input['index'], input['value'])\n",
        "interp.invoke()\n",
        "\n",
        "tflite_result = []\n",
        "for item in interp.get_output_details():\n",
        "    tflite_result.append(interp.get_tensor(item['index']))\n",
        "\n",
        "for index, (i, j) in enumerate(zip(tflite_result, result)):\n",
        "    print(\"result {} cosine = \".format(index), get_cosine(i, j))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905aedaf-591e-4d64-8e2c-f6cb3f1491b5",
      "metadata": {
        "id": "905aedaf-591e-4d64-8e2c-f6cb3f1491b5"
      },
      "source": [
        "# 6. Compile model with multiple inputs.\n",
        "\n",
        "After reading [Example of compiling model with a single input](#Example_of_compiling_model_with_a_single_input), you know how to set calib_data for a single input.<br>\n",
        "We will show you how to create a calib_data when your model has more inputs.<br>\n",
        "If model has multiple inputs, calib_data format is `[[x1, x2,...], [y1, y2,...], ...]`. <br>\n",
        "```\n",
        "e.g. Model has three inputs (x, y, z), and these inputs info are like this.\n",
        "x:{shape: [3,100], range: [1,5], dtype: int64}\n",
        "y:{shape: [100, 3, 192], range: [0,1), dtype: float32}\n",
        "z:{shape: [3,100], dtype: bool}\n",
        "\n",
        "The calib_data will be like the one below.\n",
        "\n",
        "calib_data = [\n",
        "[ np.random.randint(1, 5, size=[3,100], dtype='int64'), np.random.randint(1, 5, size=[3,100], dtype='int64')],\n",
        "[ np.random.rand(100, 3, 192).astype(np.float32), np.random.rand(100, 3, 192).astype(np.float32)],\n",
        "[ np.random.rand(3,100).astype(np.float32)>0.5, np.random.rand(3,100).astype(np.float32)>0.5],] # bool\n",
        "```\n",
        "\n",
        "Here, we will use an easier model to show you how to do it. <br>\n",
        "The model is shown below.<br>\n",
        "![image.png](attachment:562afee7-c078-4323-bc19-49e03c80d0e9.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f758da-bd81-44f9-9eff-734a23c427c2",
      "metadata": {
        "id": "73f758da-bd81-44f9-9eff-734a23c427c2"
      },
      "outputs": [],
      "source": [
        "# compile kmodel multiple inputs\n",
        "model_path = \"/content/nncase/examples/user_guide/test.onnx\"\n",
        "dump_path = \"./tmp_onnx\"\n",
        "\n",
        "# sample_count is 2\n",
        "calib_data = [[np.random.rand(1, 1, 1024).astype(np.float32), np.random.rand(1, 1, 1024).astype(np.float32)],\n",
        "              [np.random.rand(1, 1, 320).astype(np.float32), np.random.rand(1, 1, 320).astype(np.float32)]]\n",
        "\n",
        "kmodel_path = compile_kmodel(model_path, dump_path, calib_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "351e2e49-7869-4c86-a5be-e8b7412b4d06",
      "metadata": {
        "id": "351e2e49-7869-4c86-a5be-e8b7412b4d06"
      },
      "source": [
        "# 7. Simulate kmodel with multiple inputs.\n",
        "\n",
        "Simulate kmodel on PC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc1f43a-99fa-463a-b1b1-c6a9174ecd7a",
      "metadata": {
        "id": "8fc1f43a-99fa-463a-b1b1-c6a9174ecd7a"
      },
      "outputs": [],
      "source": [
        "# run kmodel(simulate)\n",
        "import os\n",
        "\n",
        "kmodel_path = \"./tmp_onnx/test.kmodel\"\n",
        "input_data = [np.random.rand(1, 1, 1024).astype(np.float32), np.random.rand(1, 1, 320).astype(np.float32)]\n",
        "\n",
        "results = run_kmodel(kmodel_path, input_data)\n",
        "\n",
        "for idx, i in enumerate(results):\n",
        "    print(i.shape)\n",
        "    i.tofile(os.path.join(dump_path,\"nncase_result_{}.bin\".format(idx)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5a79f4-0094-476f-ac79-6ea6b100b5fc",
      "metadata": {
        "id": "dd5a79f4-0094-476f-ac79-6ea6b100b5fc"
      },
      "source": [
        "# 8. Compare kmodel results and onnx results.\n",
        "\n",
        "Here, we will use the ONNX framework to infer model(`.onnx`, not kmodel). And calculate the cosine between the ONNX result and the kmodel result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee2b661d-f6eb-4dee-b014-0fd3f95d589d",
      "metadata": {
        "id": "ee2b661d-f6eb-4dee-b014-0fd3f95d589d"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime\n",
        "import onnxruntime as rt\n",
        "\n",
        "# onnx_model = model_simplify(model_path)\n",
        "onnx_model = model_path\n",
        "_, input_info = parse_model_input_output(model_path)\n",
        "onnx_sess = rt.InferenceSession(onnx_model)\n",
        "\n",
        "input_dict = {}\n",
        "for i, info in enumerate(input_info):\n",
        "    print(info['shape'])\n",
        "    input_dict[info[\"name\"]] = input_data[i]\n",
        "\n",
        "onnx_results = onnx_sess.run(None, input_dict)\n",
        "for index, (i, j) in enumerate(zip(onnx_results, results)):\n",
        "    print(\"result {} cosine = \".format(index), get_cosine(i, j))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}